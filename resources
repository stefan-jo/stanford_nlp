Videos:
	- Stanford NLP course 2017 lesson 8: Lecture 8: Recurrent Neural Networks and Language Models
	- Stanford CS231n 2016 : CS231n Winter 2016: Lecture 10: Recurrent Neural Networks, Image Captioning, LSTM
	- Stanford CS231n 2017: Lecture 10 | Recurrent Neural Networks 
	- fastai NLP course lesson 8: Intro to Language Modeling (NLP video 8)
	- fastai NLP course lesson 11: Understanding RNNs (NLP video 11)
	- Berkeley: L11 Language Models -- guest instructor: Alec Radford (OpenAI) --- Deep Unsupervised Learning SP20
	- Illustrated guide to LSTMs and GRUs: Illustrated Guide to LSTM's and GRU's: A step by step explanation
	
	
TWIML content:
	- TWIML podcast with Richard Socher on language modeling: https://twimlai.com/twiml-talk-372-language-modeling-and-protein-generation-at-salesforce-with-richard-socher/
	- TWIML with Jürgen Schmidhuber on LSTMs: https://twimlai.com/twiml-talk-44-jurgen-schmidhuber-lstms-plus-deep-learning-history-lesson/
	- TWIML podcast with Stephen Merity on SHA-RNN: https://twimlai.com/twiml-talk-325-single-headed-attention-rnn-stop-thinking-with-your-head-with-stephen-merity/
	- TWIML debate - Dissecting the Controversy around OpenAI’s New Language Model: https://twimlai.com/twiml-talk-234-dissecting-the-controversy-surrounding-openais-new-language-model/

Notebooks: 
	- fastai NLP course repo: https://github.com/fastai/course-nlp
	- fastbook draft on RNNs: https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb
	- Made with ML: https://github.com/madewithml/basics/tree/master/notebooks/15_Recurrent_Neural_Networks
	- vanishing gradients: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html

Blogposts:
	- Andrew Karpathy - unreasonable effectiveness of RNNs:  http://karpathy.github.io/2015/05/21/rnn-effectiveness/
	- Chris Olah - understanding LSTMs: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
	- Distill.pub - memorization in LSTMs: https://distill.pub/2019/memorization-in-rnns/
	- Open AI - better LMs and their implications: https://openai.com/blog/better-language-models/#sample1

Official tutorials:
	- PyTorch: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html
	- Tensorflow: https://www.tensorflow.org/guide/keras/rnn

Papers:
	- ULMFit paper: https://arxiv.org/abs/1801.06146
	- AWD-LSTM: https://arxiv.org/abs/1708.02182
	- SHA-RNN: https://arxiv.org/abs/1911.11423
	- GPT-2: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
	- GPT-3: https://arxiv.org/abs/2005.14165

Other: 
Write with transformer: https://transformer.huggingface.co/doc/gpt2-large
