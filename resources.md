Lectures:
- Stanford CS224n 2017 lecture 8 (RNNs): https://www.youtube.com/watch?v=Keqep_PKrY8&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=8
- Stanford CS231n 2016 lecture 10 (RNNs): https://www.youtube.com/watch?v=yCC09vCHzF8
- Stanford CS231n 2017 lecture 10 (RNNs): https://www.youtube.com/watch?v=6niqTuYFZLQ&t=1656s
- fastai NLP course lesson 8 (Intro to language modeling): https://www.youtube.com/watch?v=PNNHaQUQqW8
- fastai NLP course lesson 11 (Understanding RNNs): https://www.youtube.com/watch?v=l1rlFh0PmZw
- fastai NLP course lesson 15 (Implementing a GRU): https://www.youtube.com/watch?v=Bl6WVj6wQaE
- Berkeley Deep Unsupervised Learning lesson 11 (LMs): https://www.youtube.com/watch?v=BnpB3GrpsfM&t=10s
	
	
TWIML content:
- TWIML podcast with Richard Socher on language modeling: https://twimlai.com/twiml-talk-372-language-modeling-and-protein-generation-at-salesforce-with-richard-socher/
- TWIML with Jürgen Schmidhuber on LSTMs: https://twimlai.com/twiml-talk-44-jurgen-schmidhuber-lstms-plus-deep-learning-history-lesson/
- TWIML podcast with Stephen Merity on SHA-RNN: https://twimlai.com/twiml-talk-325-single-headed-attention-rnn-stop-thinking-with-your-head-with-stephen-merity/
- TWIML debate - Dissecting the Controversy around OpenAI’s New Language Model: https://twimlai.com/twiml-talk-234-dissecting-the-controversy-surrounding-openais-new-language-model/

Notebooks: 
- fastai NLP course repo: https://github.com/fastai/course-nlp
- fastbook draft on RNNs: https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb
- Made with ML: https://github.com/madewithml/basics/tree/master/notebooks/15_Recurrent_Neural_Networks
- vanishing gradients: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html

Blogposts:
- Andrew Karpathy - unreasonable effectiveness of RNNs:  http://karpathy.github.io/2015/05/21/rnn-effectiveness/
- Chris Olah - understanding LSTMs: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
- Michael Phi - Illustrated Guide to LSTMs and GRUs: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21
- Distill.pub - memorization in LSTMs: https://distill.pub/2019/memorization-in-rnns/
- Open AI - better LMs and their implications: https://openai.com/blog/better-language-models/

Official tutorials:
- PyTorch: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html
- Tensorflow: https://www.tensorflow.org/guide/keras/rnn

Papers:
- original LSTM paper: https://www.bioinf.jku.at/publications/older/2604.pdf
- original GRU paper: https://arxiv.org/abs/1406.1078
- ULMFit paper: https://arxiv.org/abs/1801.06146
- AWD-LSTM: https://arxiv.org/abs/1708.02182
- SHA-RNN: https://arxiv.org/abs/1911.11423
- GPT-3: https://arxiv.org/abs/2005.14165

Other: 
- Write with transformer: https://transformer.huggingface.co/doc/gpt2-large
- Eigenvalues explained (video): https://www.youtube.com/watch?v=G4N8vJpf7hM&t=199s
- L2 norm definition: https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-73003-5_1070#:~:text=L2%20norm%20is%20a%20standard,the%20values%20in%20each%20dimension.
